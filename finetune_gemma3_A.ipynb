{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPTw6ARLPmAXzazi3oWvHP4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eb2fac784adc47afb200ea91b0ba3c48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0e14cdbea4864ea593f48c1469697a36",
              "IPY_MODEL_8c16959339d34049a346fde652328283",
              "IPY_MODEL_121e018e89e644f898833c090dff3ab5"
            ],
            "layout": "IPY_MODEL_f12659f2249e4c668dfa1e25f95b7961"
          }
        },
        "0e14cdbea4864ea593f48c1469697a36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8160d08c13634def93fc9b4f5160ac47",
            "placeholder": "​",
            "style": "IPY_MODEL_e9f342f20b1f4aafbfed8840e0d2d77e",
            "value": "Map: 100%"
          }
        },
        "8c16959339d34049a346fde652328283": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e25d187fbec4461b96d20b5264972686",
            "max": 1101,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_14e8371ef5b3453c875aebcf31b345e6",
            "value": 1101
          }
        },
        "121e018e89e644f898833c090dff3ab5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2279c9bea9e947c5b63f01f8d977d233",
            "placeholder": "​",
            "style": "IPY_MODEL_6dd62bff3af34135a5dfe328f1980fcb",
            "value": " 1101/1101 [00:00&lt;00:00, 5865.49 examples/s]"
          }
        },
        "f12659f2249e4c668dfa1e25f95b7961": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8160d08c13634def93fc9b4f5160ac47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9f342f20b1f4aafbfed8840e0d2d77e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e25d187fbec4461b96d20b5264972686": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14e8371ef5b3453c875aebcf31b345e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2279c9bea9e947c5b63f01f8d977d233": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dd62bff3af34135a5dfe328f1980fcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d28d1cc2137943b7a0dcc56bb0a12041": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b0d0047d850643a9bd6b1c2f64de21a8",
              "IPY_MODEL_58165c869a5c4d1baa84a1d72b78117e",
              "IPY_MODEL_8dff9f430e0940799ab4391c591b9fe7"
            ],
            "layout": "IPY_MODEL_62d8bb923da0460bbe9011c17a1f52ff"
          }
        },
        "b0d0047d850643a9bd6b1c2f64de21a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66fd313c3ed44a7a8de388b34f4710b5",
            "placeholder": "​",
            "style": "IPY_MODEL_56f83b66bddb4fe2a92d8ae49a35dfd9",
            "value": "Map: 100%"
          }
        },
        "58165c869a5c4d1baa84a1d72b78117e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e33885f609444e0eadfd78b9aae7d9a2",
            "max": 990,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_efbc623ae5024384b65fd542585ab292",
            "value": 990
          }
        },
        "8dff9f430e0940799ab4391c591b9fe7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d66f43df32b94a75b0ce23ccecc0bd9e",
            "placeholder": "​",
            "style": "IPY_MODEL_2096ea81756b421ea943a3a9b3e66d60",
            "value": " 990/990 [00:00&lt;00:00, 1383.25 examples/s]"
          }
        },
        "62d8bb923da0460bbe9011c17a1f52ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66fd313c3ed44a7a8de388b34f4710b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56f83b66bddb4fe2a92d8ae49a35dfd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e33885f609444e0eadfd78b9aae7d9a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efbc623ae5024384b65fd542585ab292": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d66f43df32b94a75b0ce23ccecc0bd9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2096ea81756b421ea943a3a9b3e66d60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a2c08f2b8ef4499b4f1994e3f36f681": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_84b0b536b30a4d9c915c3b47ba22903f",
              "IPY_MODEL_67f0475883fb4ac2aac87774a9bdc968",
              "IPY_MODEL_61fb4c0d2f174290ab8871e34f021e4f"
            ],
            "layout": "IPY_MODEL_36f2ad44a7664019ac5be82840de78b8"
          }
        },
        "84b0b536b30a4d9c915c3b47ba22903f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b1f9085720046b198a68b2448acd6bc",
            "placeholder": "​",
            "style": "IPY_MODEL_82a4f97677ac46c8a8ffb2c1b76bd003",
            "value": "Map: 100%"
          }
        },
        "67f0475883fb4ac2aac87774a9bdc968": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6722054e5aa400399f7d9e1bec909b1",
            "max": 111,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f61e247f2c74d5bb474d9dd63bfa781",
            "value": 111
          }
        },
        "61fb4c0d2f174290ab8871e34f021e4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc1c858c459a491fa52a06a54e7f8cf5",
            "placeholder": "​",
            "style": "IPY_MODEL_999a22c7a2ff4e7bb56c594c30a56186",
            "value": " 111/111 [00:00&lt;00:00, 435.54 examples/s]"
          }
        },
        "36f2ad44a7664019ac5be82840de78b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b1f9085720046b198a68b2448acd6bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82a4f97677ac46c8a8ffb2c1b76bd003": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6722054e5aa400399f7d9e1bec909b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f61e247f2c74d5bb474d9dd63bfa781": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fc1c858c459a491fa52a06a54e7f8cf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "999a22c7a2ff4e7bb56c594c30a56186": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41c6db57229241e09aea266c20b6dafc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9d2a5b810fbf4cf29e15c3224e6bc275",
              "IPY_MODEL_50aefa5890f346a481a87e5a7ce4e5da",
              "IPY_MODEL_ab56080b89ec433e9eaa614757a20528"
            ],
            "layout": "IPY_MODEL_b18aae567d07458681187e237b0105de"
          }
        },
        "9d2a5b810fbf4cf29e15c3224e6bc275": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9c5a29ddeee4c16b1205bfb7437bf8f",
            "placeholder": "​",
            "style": "IPY_MODEL_bed99822004a41bdaa8229e57e039016",
            "value": "Unsloth: Tokenizing [&quot;text&quot;] (num_proc=6): 100%"
          }
        },
        "50aefa5890f346a481a87e5a7ce4e5da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f333431cb804399987c4236e6c989ae",
            "max": 990,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fa061785f3324d7499e5aff76bbd3699",
            "value": 990
          }
        },
        "ab56080b89ec433e9eaa614757a20528": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a73dc2eaba8402eb33d958348cf47e1",
            "placeholder": "​",
            "style": "IPY_MODEL_50aae34f43834e9188c87f5719aeb050",
            "value": " 990/990 [00:13&lt;00:00, 125.46 examples/s]"
          }
        },
        "b18aae567d07458681187e237b0105de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9c5a29ddeee4c16b1205bfb7437bf8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bed99822004a41bdaa8229e57e039016": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f333431cb804399987c4236e6c989ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa061785f3324d7499e5aff76bbd3699": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0a73dc2eaba8402eb33d958348cf47e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50aae34f43834e9188c87f5719aeb050": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73307d75c59c42e495bea9fe72d02e18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e43ad60b2bb4400b79ca673d0ae3e5b",
              "IPY_MODEL_8901d785e6364559a64a8c9bcdc6e026",
              "IPY_MODEL_d20434e377cd4e789eac34cda0bb5569"
            ],
            "layout": "IPY_MODEL_bc8b985164374fc8a1b5b595e9dad0e4"
          }
        },
        "6e43ad60b2bb4400b79ca673d0ae3e5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e368430e79e442369db0636cb74bc5db",
            "placeholder": "​",
            "style": "IPY_MODEL_15793b7990e7449a8092faa632c9a4a6",
            "value": "Unsloth: Tokenizing [&quot;text&quot;] (num_proc=6): 100%"
          }
        },
        "8901d785e6364559a64a8c9bcdc6e026": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a212d6df6a14372b5a94d3677aa5c99",
            "max": 111,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fd039b3c3b66472eba8d2d8806d4ba6e",
            "value": 111
          }
        },
        "d20434e377cd4e789eac34cda0bb5569": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f1ca3da14974353ab3a6e19aedae959",
            "placeholder": "​",
            "style": "IPY_MODEL_a21fbc89e02140abb7755302be24fb04",
            "value": " 111/111 [00:14&lt;00:00, 12.73 examples/s]"
          }
        },
        "bc8b985164374fc8a1b5b595e9dad0e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e368430e79e442369db0636cb74bc5db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15793b7990e7449a8092faa632c9a4a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a212d6df6a14372b5a94d3677aa5c99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd039b3c3b66472eba8d2d8806d4ba6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f1ca3da14974353ab3a6e19aedae959": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a21fbc89e02140abb7755302be24fb04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vardhin/WeLearn/blob/master/finetune_gemma3_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 897,
          "referenced_widgets": [
            "eb2fac784adc47afb200ea91b0ba3c48",
            "0e14cdbea4864ea593f48c1469697a36",
            "8c16959339d34049a346fde652328283",
            "121e018e89e644f898833c090dff3ab5",
            "f12659f2249e4c668dfa1e25f95b7961",
            "8160d08c13634def93fc9b4f5160ac47",
            "e9f342f20b1f4aafbfed8840e0d2d77e",
            "e25d187fbec4461b96d20b5264972686",
            "14e8371ef5b3453c875aebcf31b345e6",
            "2279c9bea9e947c5b63f01f8d977d233",
            "6dd62bff3af34135a5dfe328f1980fcb",
            "d28d1cc2137943b7a0dcc56bb0a12041",
            "b0d0047d850643a9bd6b1c2f64de21a8",
            "58165c869a5c4d1baa84a1d72b78117e",
            "8dff9f430e0940799ab4391c591b9fe7",
            "62d8bb923da0460bbe9011c17a1f52ff",
            "66fd313c3ed44a7a8de388b34f4710b5",
            "56f83b66bddb4fe2a92d8ae49a35dfd9",
            "e33885f609444e0eadfd78b9aae7d9a2",
            "efbc623ae5024384b65fd542585ab292",
            "d66f43df32b94a75b0ce23ccecc0bd9e",
            "2096ea81756b421ea943a3a9b3e66d60",
            "6a2c08f2b8ef4499b4f1994e3f36f681",
            "84b0b536b30a4d9c915c3b47ba22903f",
            "67f0475883fb4ac2aac87774a9bdc968",
            "61fb4c0d2f174290ab8871e34f021e4f",
            "36f2ad44a7664019ac5be82840de78b8",
            "5b1f9085720046b198a68b2448acd6bc",
            "82a4f97677ac46c8a8ffb2c1b76bd003",
            "e6722054e5aa400399f7d9e1bec909b1",
            "8f61e247f2c74d5bb474d9dd63bfa781",
            "fc1c858c459a491fa52a06a54e7f8cf5",
            "999a22c7a2ff4e7bb56c594c30a56186",
            "41c6db57229241e09aea266c20b6dafc",
            "9d2a5b810fbf4cf29e15c3224e6bc275",
            "50aefa5890f346a481a87e5a7ce4e5da",
            "ab56080b89ec433e9eaa614757a20528",
            "b18aae567d07458681187e237b0105de",
            "c9c5a29ddeee4c16b1205bfb7437bf8f",
            "bed99822004a41bdaa8229e57e039016",
            "5f333431cb804399987c4236e6c989ae",
            "fa061785f3324d7499e5aff76bbd3699",
            "0a73dc2eaba8402eb33d958348cf47e1",
            "50aae34f43834e9188c87f5719aeb050",
            "73307d75c59c42e495bea9fe72d02e18",
            "6e43ad60b2bb4400b79ca673d0ae3e5b",
            "8901d785e6364559a64a8c9bcdc6e026",
            "d20434e377cd4e789eac34cda0bb5569",
            "bc8b985164374fc8a1b5b595e9dad0e4",
            "e368430e79e442369db0636cb74bc5db",
            "15793b7990e7449a8092faa632c9a4a6",
            "2a212d6df6a14372b5a94d3677aa5c99",
            "fd039b3c3b66472eba8d2d8806d4ba6e",
            "4f1ca3da14974353ab3a6e19aedae959",
            "a21fbc89e02140abb7755302be24fb04"
          ]
        },
        "id": "7CwdPwSGx2HD",
        "outputId": "08ecc44f-ef49-4f22-df7a-b491c3329938"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Compatible NVIDIA GPU detected.\n",
            "✅ Dependencies are already installed.\n",
            "Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Google Drive mounted successfully at /content/drive.\n",
            "Loading model and tokenizer...\n",
            "==((====))==  Unsloth 2025.9.6: Fast Gemma3_Text patching. Transformers: 4.55.4.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Using float16 precision for gemma3_text won't work! Using float32.\n",
            "Applying Gemma-3 chat template...\n",
            "Configuring LoRA...\n",
            "Unsloth: Making `model.base_model.model.model` require gradients\n",
            "Loading and preparing dataset from /content/drive/MyDrive/MLjournal/trainingA.jsonl...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1101 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb2fac784adc47afb200ea91b0ba3c48"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset split into 990 training and 111 evaluation samples.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/990 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d28d1cc2137943b7a0dcc56bb0a12041"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/111 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a2c08f2b8ef4499b4f1994e3f36f681"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Switching to float32 training since model cannot work with float16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/990 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41c6db57229241e09aea266c20b6dafc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/111 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "73307d75c59c42e495bea9fe72d02e18"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 990 | Num Epochs = 8 | Total steps = 992\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 30,375,936 of 298,474,112 (10.18% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training... 🚀\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='992' max='992' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [992/992 23:22, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.357100</td>\n",
              "      <td>2.901191</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Not an error, but Gemma3ForCausalLM does not accept `num_items_in_batch`.\n",
            "Using gradient accumulation will be very slightly less accurate.\n",
            "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving final LoRA adapters to /content/drive/MyDrive/gemma3-270m-finetuned...\n",
            "Attempting to save model in GGUF format...\n",
            "Unsloth: Updating system package directories\n",
            "Unsloth: Install GGUF and other packages\n",
            "Could not save GGUF model. This might be due to model compatibility or library versions. Error: Unsloth: `config.json` does not exist inside `/content/drive/MyDrive/gemma3-270m-finetuned`.\n",
            "\n",
            "🎉 Training completed! Model and tokenizer saved to /content/drive/MyDrive/gemma3-270m-finetuned\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "from datasets import Dataset, load_dataset\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# --- Colab Specific Setup ---\n",
        "\n",
        "def check_gpu_compatibility():\n",
        "    \"\"\"\n",
        "    Checks for a compatible NVIDIA GPU and exits if one is not found.\n",
        "    \"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"\\n\\n\" + \"=\"*80)\n",
        "        print(\"❌ ERROR: No NVIDIA GPU detected. Unsloth requires a GPU.\")\n",
        "        print(\"Please change your runtime type to a GPU.\")\n",
        "        print(\"Go to 'Runtime' -> 'Change runtime type' -> 'Hardware accelerator' and select 'T4 GPU' or another GPU.\")\n",
        "        print(\"=\"*80 + \"\\n\\n\")\n",
        "        os.kill(os.getpid(), 9)\n",
        "    else:\n",
        "        print(\"✅ Compatible NVIDIA GPU detected.\")\n",
        "\n",
        "def check_and_install_dependencies():\n",
        "    \"\"\"\n",
        "    Checks if unsloth is installed. If not, it installs all required dependencies\n",
        "    and then stops execution to allow for a manual runtime restart.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import unsloth\n",
        "        print(\"✅ Dependencies are already installed.\")\n",
        "    except ImportError:\n",
        "        print(\"Installing necessary packages. This may take a few minutes...\")\n",
        "\n",
        "        try:\n",
        "            # Using subprocess to handle installation quietly\n",
        "            # Unsloth's Gemma-3 support requires newer versions of dependencies\n",
        "            subprocess.run([\"pip\", \"install\", \"unsloth[colab-new]\"], check=True, capture_output=True)\n",
        "            subprocess.run([\"pip\", \"install\", \"--no-deps\", \"trl\", \"peft\", \"accelerate\", \"bitsandbytes\"], check=True, capture_output=True)\n",
        "            print(\"✅ Installation successful.\")\n",
        "\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"❌ An error occurred during installation: {e}\")\n",
        "            print(e.stderr.decode())\n",
        "            # Exit if installation fails\n",
        "            exit()\n",
        "\n",
        "        # Stop execution to force user to restart runtime\n",
        "        print(\"\\n\\n\" + \"=\"*80)\n",
        "        print(\"IMPORTANT: Dependencies installed. You MUST restart the runtime now.\")\n",
        "        print(\"Go to 'Runtime' -> 'Restart runtime' in the menu, and then run this cell again.\")\n",
        "        print(\"=\"*80 + \"\\n\\n\")\n",
        "        os.kill(os.getpid(), 9)\n",
        "\n",
        "def mount_google_drive():\n",
        "    \"\"\"Mounts Google Drive to the Colab environment.\"\"\"\n",
        "    print(\"Mounting Google Drive...\")\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"✅ Google Drive mounted successfully at /content/drive.\")\n",
        "    except ImportError:\n",
        "        print(\"This script is designed to run in Google Colab. Could not find google.colab library.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while mounting Google Drive: {e}\")\n",
        "        exit()\n",
        "\n",
        "# Run setup tasks first\n",
        "check_gpu_compatibility()\n",
        "check_and_install_dependencies()\n",
        "mount_google_drive()\n",
        "\n",
        "# Now that setup is done, we can import the heavy libraries\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# --- Core Functions ---\n",
        "\n",
        "def load_jsonl_dataset(file_path):\n",
        "    \"\"\"Load and prepare dataset from JSONL file.\"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"Dataset file not found at: {file_path}. Please ensure the file exists in your Google Drive.\")\n",
        "    data = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line.strip()))\n",
        "    return data\n",
        "\n",
        "def format_conversation_structure(example):\n",
        "    \"\"\"\n",
        "    Formats an example from various known structures into a unified\n",
        "    list of messages format. This format is compatible with apply_chat_template.\n",
        "    \"\"\"\n",
        "    messages = []\n",
        "    if 'instruction' in example and 'output' in example:\n",
        "        # Alpaca-style format\n",
        "        messages.append({\"role\": \"user\", \"content\": example['instruction']})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": example['output']})\n",
        "    elif 'conversations' in example:\n",
        "        # ShareGPT-style format\n",
        "        for message in example['conversations']:\n",
        "            role = \"user\" if message.get('from') == 'human' else \"assistant\"\n",
        "            content = message.get('value', '')\n",
        "            messages.append({\"role\": role, \"content\": content})\n",
        "    # Add more format checks here if needed\n",
        "    else:\n",
        "        raise ValueError(\"Dataset format not recognized. Expected 'instruction'+'output' or 'conversations' format.\")\n",
        "\n",
        "    return {\"messages\": messages}\n",
        "\n",
        "def main():\n",
        "    # --- Configuration ---\n",
        "    # ⭐️ MODIFIED: Updated model name to Gemma-3 270M\n",
        "    model_name = \"unsloth/gemma-3-270m-it-bnb-4bit\"\n",
        "    # ⚠️ IMPORTANT: Update this path to point to your training data in Google Drive\n",
        "    jsonl_file_path = \"/content/drive/MyDrive/MLjournal/trainingA.jsonl\"\n",
        "    # ⭐️ MODIFIED: Updated output directory for the new model\n",
        "    output_dir = \"/content/drive/MyDrive/gemma3-270m-finetuned\"\n",
        "    max_seq_length = 2048\n",
        "\n",
        "    # --- Model and Tokenizer Loading ---\n",
        "    print(\"Loading model and tokenizer...\")\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=model_name,\n",
        "        max_seq_length=max_seq_length,\n",
        "        dtype=None,      # Auto-detect\n",
        "        load_in_4bit=True, # This enables QLoRA quantization\n",
        "    )\n",
        "\n",
        "    # --- ⭐️ CRITICAL FOR GEMMA-3: Apply the correct chat template ---\n",
        "    print(\"Applying Gemma-3 chat template...\")\n",
        "    tokenizer = get_chat_template(\n",
        "        tokenizer,\n",
        "        chat_template=\"gemma3\",\n",
        "    )\n",
        "\n",
        "    # --- LoRA Configuration ---\n",
        "    print(\"Configuring LoRA...\")\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        # ⭐️ MODIFIED: Increased rank and alpha for potentially better performance\n",
        "        r=128,\n",
        "        lora_alpha=128,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "        use_gradient_checkpointing=\"unsloth\",\n",
        "        random_state=3407,\n",
        "        use_rslora=False,\n",
        "        loftq_config=None,\n",
        "    )\n",
        "\n",
        "    # --- Dataset Preparation ---\n",
        "    print(f\"Loading and preparing dataset from {jsonl_file_path}...\")\n",
        "    raw_data = load_jsonl_dataset(jsonl_file_path)\n",
        "    dataset = Dataset.from_list(raw_data)\n",
        "\n",
        "    # 1. First, map to create the 'messages' column with a standardized conversation structure\n",
        "    structured_dataset = dataset.map(format_conversation_structure, remove_columns=list(dataset.features))\n",
        "\n",
        "    # --- Dataset Splitting ---\n",
        "    if len(structured_dataset) > 100:\n",
        "        train_test_split = structured_dataset.train_test_split(test_size=0.1)\n",
        "        train_dataset = train_test_split['train']\n",
        "        eval_dataset = train_test_split['test']\n",
        "        print(f\"Dataset split into {len(train_dataset)} training and {len(eval_dataset)} evaluation samples.\")\n",
        "    else:\n",
        "        train_dataset = structured_dataset\n",
        "        eval_dataset = None\n",
        "        print(f\"Using the full dataset of {len(train_dataset)} samples for training.\")\n",
        "\n",
        "\n",
        "    # 2. Define a formatting function that uses the tokenizer to create the final training string.\n",
        "    def formatting_prompts_func(example):\n",
        "        # The .removeprefix('<bos>') is a recommended practice from the Unsloth Gemma-3 notebook\n",
        "        text = tokenizer.apply_chat_template(example[\"messages\"], tokenize=False, add_generation_prompt=False).removeprefix('<bos>')\n",
        "        return {\"text\": text}\n",
        "\n",
        "    # 3. Map this function to create the final 'text' column that the SFTTrainer will use\n",
        "    train_dataset = train_dataset.map(formatting_prompts_func)\n",
        "    if eval_dataset:\n",
        "        eval_dataset = eval_dataset.map(formatting_prompts_func)\n",
        "\n",
        "    # --- Training Arguments ---\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=8,\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=100,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=10,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=500,\n",
        "        save_total_limit=2,\n",
        "        eval_strategy=\"steps\" if eval_dataset else \"no\",\n",
        "        eval_steps=500 if eval_dataset else None,\n",
        "        load_best_model_at_end=True if eval_dataset else False,\n",
        "        report_to=\"none\",  # Disable wandb logging\n",
        "    )\n",
        "\n",
        "    # --- Initialize Trainer ---\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        dataset_text_field=\"text\", # **CRITICAL**: Point to the new 'text' column\n",
        "        max_seq_length=max_seq_length,\n",
        "        args=training_args,\n",
        "        dataset_num_proc=2, # Enable multiprocessing for faster data processing\n",
        "    )\n",
        "\n",
        "    # --- Start Training ---\n",
        "    print(\"Starting training... 🚀\")\n",
        "    trainer.train()\n",
        "\n",
        "    # --- Save Final Model ---\n",
        "    print(f\"Saving final LoRA adapters to {output_dir}...\")\n",
        "    trainer.save_model()\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    # --- Save to GGUF Format (Optional) ---\n",
        "    print(\"Attempting to save model in GGUF format...\")\n",
        "    try:\n",
        "        # ⭐️ MODIFIED: Using `quantization_type` as per the latest Unsloth API for GGUF\n",
        "        model.save_pretrained_gguf(output_dir, tokenizer, quantization_type=\"Q8_0\")\n",
        "        print(\"✅ GGUF model (Q8_0) saved successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not save GGUF model. This might be due to model compatibility or library versions. Error: {e}\")\n",
        "\n",
        "    print(f\"\\n🎉 Training completed! Model and tokenizer saved to {output_dir}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Start the main training process\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from unsloth import FastLanguageModel\n",
        "import gc\n",
        "\n",
        "# --- Configuration ---\n",
        "# MODIFIED: Switched the model to a 4bit-quantized Gemma 2B Instruct model\n",
        "base_model_name = \"unsloth/gemma-3-270m-it-bnb-4bit\"\n",
        "jsonl_file_path = \"/content/drive/MyDrive/MLjournal/trainingA.jsonl\" # ⚠️ Make sure this path is correct\n",
        "NUM_TEST_QUESTIONS = 5\n",
        "\n",
        "# --- 1. Load Dataset and Create a Sample ---\n",
        "print(\"Loading dataset and preparing sample...\")\n",
        "try:\n",
        "    with open(jsonl_file_path, 'r', encoding='utf-8') as f:\n",
        "        data = [json.loads(line) for line in f]\n",
        "    dataset = Dataset.from_list(data)\n",
        "    sample_dataset = dataset.select(range(NUM_TEST_QUESTIONS))\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ ERROR: Dataset not found at '{jsonl_file_path}'. Please check the file path.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# --- 2. Load the Base Model ---\n",
        "print(f\"Loading base model: {base_model_name}...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=base_model_name,\n",
        "    dtype=None,      # Let Unsloth auto-detect\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "\n",
        "# --- 3. Generate and Print Responses ---\n",
        "print(f\"\\n--- Generating responses from BASE MODEL ({base_model_name}) ---\")\n",
        "for item in sample_dataset:\n",
        "    # This logic correctly handles both 'instruction' and 'conversations' formats\n",
        "    prompt = \"\"\n",
        "    original_output = \"\"\n",
        "    if 'instruction' in item:\n",
        "        prompt = item.get('instruction', '')\n",
        "        original_output = item.get('output', '')\n",
        "    elif 'conversations' in item:\n",
        "        if len(item['conversations']) > 0 and item['conversations'][0]['from'] == 'human':\n",
        "            prompt = item['conversations'][0]['value']\n",
        "            if len(item['conversations']) > 1:\n",
        "                original_output = item['conversations'][1]['value']\n",
        "\n",
        "    if not prompt:\n",
        "        print(\"Skipping item with no valid prompt.\")\n",
        "        continue\n",
        "\n",
        "    # Format the prompt for inference. Unsloth's FastLanguageModel configures the\n",
        "    # tokenizer to use the correct chat template for Gemma automatically.\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
        "\n",
        "    # Generate the response\n",
        "    outputs = model.generate(inputs, max_new_tokens=128, use_cache=True, pad_token_id=tokenizer.eos_token_id)\n",
        "    response = tokenizer.batch_decode(outputs[:, inputs.shape[1]:], skip_special_tokens=True)[0]\n",
        "\n",
        "    # Print the comparison\n",
        "    print(f\"❓ PROMPT:\\n{prompt}\")\n",
        "    print(\"-------------------------------------------------\")\n",
        "    print(f\"🤖 BASE MODEL RESPONSE:\\n{response}\")\n",
        "    print(\"-------------------------------------------------\")\n",
        "    print(f\"✅ ORIGINAL RESPONSE:\\n{original_output}\")\n",
        "    print(\"=================================================\\n\")\n",
        "\n",
        "\n",
        "# --- 4. IMPORTANT: Clear memory ---\n",
        "print(\"Clearing base model from memory...\")\n",
        "del model\n",
        "del tokenizer\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"Memory cleared.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkl-FnhZ-5x2",
        "outputId": "70552887-4173-42ca-d5df-f3d657f9e4a8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "Loading dataset and preparing sample...\n",
            "Loading base model: unsloth/gemma-3-270m-it-bnb-4bit...\n",
            "==((====))==  Unsloth 2025.9.6: Fast Gemma3_Text patching. Transformers: 4.55.4.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Using float16 precision for gemma3_text won't work! Using float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generating responses from BASE MODEL (unsloth/gemma-3-270m-it-bnb-4bit) ---\n",
            "❓ PROMPT:\n",
            "What are the three anomalies observed in the Mixture-of-Experts (MoE) paradigm at Kuaishou that negatively impact model performance?\n",
            "-------------------------------------------------\n",
            "🤖 BASE MODEL RESPONSE:\n",
            "The three anomalies observed in the Mixture-of-Experts (MoE) paradigm at Kuaishou are:\n",
            "\n",
            "1.  **Data Type Assumption:**  The core anomaly arises from the assumption of a mixture of experts (i.e., users, the models, and the model's behavior) as being *only* based on the expert's suggestions.  The assumption of a single expert's suggestion is a major problem.  Specifically, the suggestion for the experts is often not explicitly stated in the input data. This leads to the following:\n",
            "\n",
            "    *   **Reduced Model Capacity:** The models trained on the\n",
            "-------------------------------------------------\n",
            "✅ ORIGINAL RESPONSE:\n",
            "The three anomalies observed are: (1) Expert Collapse, where some experts have significantly different output distributions and excessive zero activations, making it difficult for the gate networks to assign fair weights. (2) Expert Degradation, where shared experts are dominated by only one task, losing their ability to provide predictive information for all tasks. (3) Expert Underfitting, where data-sparse prediction tasks tend to ignore their specific experts and rely heavily on shared experts.\n",
            "=================================================\n",
            "\n",
            "❓ PROMPT:\n",
            "What is the main goal of the HoME (Hierarchy of Multi-Gate Experts) model proposed in the paper?\n",
            "-------------------------------------------------\n",
            "🤖 BASE MODEL RESPONSE:\n",
            "The main goal of the HoME (Hierarchy of Multi-Gate Experts) model is to **provide a more robust and robust platform for identifying and understanding the contribution of different specialized experts in [Specific Area of the Research],** particularly in [Specific Speciality].\n",
            "\n",
            "It aims to:\n",
            "\n",
            "* **Enhance the accuracy and reliability of expert identification:** By grouping experts based on their specific expertise, the model can improve the accuracy of identifying non-expert, and sometimes even misidentifying the contributions of specialists.\n",
            "* **Foster a broader and more inclusive understanding of the field:** By emphasizing the role of diverse experts, the model can\n",
            "-------------------------------------------------\n",
            "✅ ORIGINAL RESPONSE:\n",
            "The main goal of HoME is to achieve a simple, efficient, and balanced MoE system for multi-task learning that addresses the observed anomalies (expert collapse, degradation, and underfitting) to improve the stability and performance of the model.\n",
            "=================================================\n",
            "\n",
            "❓ PROMPT:\n",
            "Describe the \"Expert normalization & Swish mechanism\" used in HoME and explain why it is implemented.\n",
            "-------------------------------------------------\n",
            "🤖 BASE MODEL RESPONSE:\n",
            "The \"Expert Normalization & Swish Mechanism\" in HoME is a crucial mechanism for achieving efficient and accurate knowledge sharing and reducing the impact of noisy and irrelevant data. It's implemented to address the following:\n",
            "\n",
            "*   **Handling of Ambiguous, Misleading, and Incomplete Data:**\n",
            "    *   **Ambiguous Data:**  By using explicit knowledge representation (e.g., explicit knowledge bases, knowledge graphs, and user-defined knowledge bases), we can effectively handle ambiguous data.  This prevents the \"Expert Normalization\" process from becoming overly complex and triggering the \"Swish Mechanism\"  by providing more contextual\n",
            "-------------------------------------------------\n",
            "✅ ORIGINAL RESPONSE:\n",
            "The \"Expert normalization & Swish mechanism\" consists of two parts. First, each expert's output is normalized to approximate a standard normal distribution (mean of 0 and unit variance). This is done to balance the variance of expert outputs and prevent expert collapse. Secondly, the ReLU activation function is replaced with the Swish function to alleviate the zero derivatives gradient phenomenon that arises after normalization, improving parameter utilization and speeding up training.\n",
            "=================================================\n",
            "\n",
            "❓ PROMPT:\n",
            "Explain the \"Hierarchy mask mechanism\" in HoME and its purpose.\n",
            "-------------------------------------------------\n",
            "🤖 BASE MODEL RESPONSE:\n",
            "The \"Hierarchy Mask Mechanism\" (HMM) in HoME is a crucial mechanism for addressing memory issues, particularly in Memory-Less and Memory-Rich environments, and in managing Memory-Uncreatable (MUC) states. It's a key to reducing memory clutter and enhancing memory performance. Here's a breakdown of the concept:\n",
            "\n",
            "**Core Idea:**\n",
            "\n",
            "The Hierarchy Mask Mechanism addresses the memory gap by forcing the user to explicitly define which elements of memory are \"important\" and \"important *only* when applicable\" when interacting with a memory-uncreatable (MUC) state. This \"is-also\n",
            "-------------------------------------------------\n",
            "✅ ORIGINAL RESPONSE:\n",
            "The \"Hierarchy mask mechanism\" introduces a pre-order meta expert network to group tasks based on prior relevance. In the Kuaishou example, tasks are divided into passive watching-time tasks and proactive interaction tasks. Each task then benefits from both fully-shared global experts and partially-shared in-category experts. The purpose is to reduce expert occupancy issues and mitigate expert degradation by allowing tasks to share experts within their category.\n",
            "=================================================\n",
            "\n",
            "❓ PROMPT:\n",
            "Describe the \"Feature-gate and Self-gate mechanisms\" and how they address the identified issues.\n",
            "-------------------------------------------------\n",
            "🤖 BASE MODEL RESPONSE:\n",
            "Feature-gate and Self-gate mechanisms are mechanisms that attempt to address the problem of \"Feature-gate\" and \"Self-gate\" in Machine Learning (ML) models. They aim to address the issue by integrating the strengths of both feature-gate and Self-gate mechanisms to achieve a more robust, efficient, and accurate ML model.\n",
            "\n",
            "Here's a breakdown of the feature-gate and Self-gate mechanisms:\n",
            "\n",
            "**Feature-gate mechanisms:**\n",
            "\n",
            "*   These mechanisms are designed to make the input data explicitly features.\n",
            "*   They directly try to provide each input with a known, labeled, and often a pre\n",
            "-------------------------------------------------\n",
            "✅ ORIGINAL RESPONSE:\n",
            "The \"Feature-gate mechanism\" provides private, flexible input to each expert to protect sparse-task expert training and prevent gradient conflicts among experts. The \"Self-gate mechanism\" is implemented by adding residual connections between adjacent related experts in deeper MMoE systems to ensure the top layers' gradient can effectively be passed to the bottom layers and stabilize the training of sparse-task experts, preventing gradient dilution.\n",
            "=================================================\n",
            "\n",
            "Clearing base model from memory...\n",
            "Memory cleared.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from unsloth import FastLanguageModel\n",
        "import gc\n",
        "\n",
        "# --- Configuration ---\n",
        "# The base model you used for fine-tuning\n",
        "base_model_name = \"unsloth/gemma-3-270m-it-bnb-4bit\"\n",
        "# The path to your saved adapter (the result of your fine-tuning)\n",
        "# ⚠️ Make sure this adapter was trained on the 'base_model_name' above.\n",
        "adapter_path = \"/content/drive/MyDrive/gemma3-270m-finetuned\"\n",
        "# The path to your dataset for testing\n",
        "jsonl_file_path = \"/content/drive/MyDrive/MLjournal/trainingA.jsonl\"\n",
        "NUM_TEST_QUESTIONS = 5\n",
        "\n",
        "# --- 1. Load Dataset and Create a Sample ---\n",
        "print(\"Loading dataset and preparing sample...\")\n",
        "try:\n",
        "    with open(jsonl_file_path, 'r', encoding='utf-8') as f:\n",
        "        data = [json.loads(line) for line in f]\n",
        "    dataset = Dataset.from_list(data)\n",
        "    sample_dataset = dataset.select(range(NUM_TEST_QUESTIONS))\n",
        "    print(f\"✅ Loaded {len(dataset)} records. Using {NUM_TEST_QUESTIONS} for testing.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ ERROR: Dataset not found at '{jsonl_file_path}'. Please check the file path.\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Load the Base Model ---\n",
        "print(f\"\\nLoading base model: {base_model_name}...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=base_model_name,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "print(\"✅ Base model loaded.\")\n",
        "\n",
        "# --- 3. Generate and Print Responses from BASE MODEL ---\n",
        "print(f\"\\n--- Generating responses from BASE MODEL ({base_model_name}) ---\")\n",
        "for item in sample_dataset:\n",
        "    # This logic handles both 'instruction' and 'conversations' formats\n",
        "    prompt = \"\"\n",
        "    original_output = \"\"\n",
        "    if 'instruction' in item:\n",
        "        prompt = item.get('instruction', '')\n",
        "        original_output = item.get('output', '')\n",
        "    elif 'conversations' in item:\n",
        "        if len(item['conversations']) > 0 and item['conversations'][0]['from'] == 'human':\n",
        "            prompt = item['conversations'][0]['value']\n",
        "            if len(item['conversations']) > 1:\n",
        "                original_output = item['conversations'][1]['value']\n",
        "\n",
        "    if not prompt:\n",
        "        print(\"Skipping item with no valid prompt.\")\n",
        "        continue\n",
        "\n",
        "    # Format the prompt for inference\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
        "\n",
        "    # Generate the response\n",
        "    outputs = model.generate(inputs, max_new_tokens=128, use_cache=True, pad_token_id=tokenizer.eos_token_id)\n",
        "    response = tokenizer.batch_decode(outputs[:, inputs.shape[1]:], skip_special_tokens=True)[0]\n",
        "\n",
        "    # Print the comparison\n",
        "    print(f\"\\n❓ PROMPT:\\n{prompt}\")\n",
        "    print(\"-------------------------------------------------\")\n",
        "    print(f\"🤖 BASE MODEL RESPONSE:\\n{response}\")\n",
        "    print(\"-------------------------------------------------\")\n",
        "    print(f\"✅ ORIGINAL RESPONSE:\\n{original_output}\")\n",
        "    print(\"=================================================\")\n",
        "\n",
        "# --- 4. IMPORTANT: Clear Base Model from Memory ---\n",
        "print(\"\\nClearing base model from memory...\")\n",
        "del model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"✅ Memory cleared.\")\n",
        "\n",
        "# --- 5. Load the Fine-Tuned Model (Base Model + Adapter) ---\n",
        "print(f\"\\nLoading fine-tuned model from: {adapter_path}...\")\n",
        "ft_model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=adapter_path, # ⬅️ Loading your adapter\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "print(\"✅ Fine-tuned model loaded.\")\n",
        "\n",
        "# --- 6. Generate and Print Responses from FINE-TUNED MODEL ---\n",
        "print(\"\\n--- Generating responses from FINE-TUNED MODEL ---\")\n",
        "for item in sample_dataset: # Using the SAME sample_dataset for a fair comparison\n",
        "    # Logic to extract prompt and original output\n",
        "    prompt = \"\"\n",
        "    original_output = \"\"\n",
        "    if 'instruction' in item:\n",
        "        prompt = item.get('instruction', '')\n",
        "        original_output = item.get('output', '')\n",
        "    elif 'conversations' in item:\n",
        "        if len(item['conversations']) > 0 and item['conversations'][0]['from'] == 'human':\n",
        "            prompt = item['conversations'][0]['value']\n",
        "            if len(item['conversations']) > 1:\n",
        "                original_output = item['conversations'][1]['value']\n",
        "\n",
        "    if not prompt:\n",
        "        print(\"Skipping item with no valid prompt.\")\n",
        "        continue\n",
        "\n",
        "    # Format the prompt for inference\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
        "\n",
        "    # Generate the response from the fine-tuned model\n",
        "    outputs = ft_model.generate(inputs, max_new_tokens=128, use_cache=True, pad_token_id=tokenizer.eos_token_id)\n",
        "    response = tokenizer.batch_decode(outputs[:, inputs.shape[1]:], skip_special_tokens=True)[0]\n",
        "\n",
        "    # Print the comparison\n",
        "    print(f\"\\n❓ PROMPT:\\n{prompt}\")\n",
        "    print(\"-------------------------------------------------\")\n",
        "    print(f\"🚀 FINE-TUNED MODEL RESPONSE:\\n{response}\")\n",
        "    print(\"-------------------------------------------------\")\n",
        "    print(f\"✅ ORIGINAL RESPONSE:\\n{original_output}\")\n",
        "    print(\"=================================================\")\n",
        "\n",
        "# --- 7. Final Cleanup ---\n",
        "print(\"\\nClearing fine-tuned model from memory...\")\n",
        "del ft_model\n",
        "del tokenizer\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"✅ All done. Memory cleared.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clkh1wPv-7J4",
        "outputId": "a0fb7608-5952-49f3-a29f-334a73e76f18"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset and preparing sample...\n",
            "✅ Loaded 1101 records. Using 5 for testing.\n",
            "\n",
            "Loading base model: unsloth/gemma-3-270m-it-bnb-4bit...\n",
            "==((====))==  Unsloth 2025.9.6: Fast Gemma3_Text patching. Transformers: 4.55.4.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Using float16 precision for gemma3_text won't work! Using float32.\n",
            "✅ Base model loaded.\n",
            "\n",
            "--- Generating responses from BASE MODEL (unsloth/gemma-3-270m-it-bnb-4bit) ---\n",
            "\n",
            "❓ PROMPT:\n",
            "What are the three anomalies observed in the Mixture-of-Experts (MoE) paradigm at Kuaishou that negatively impact model performance?\n",
            "-------------------------------------------------\n",
            "🤖 BASE MODEL RESPONSE:\n",
            "The Three-Aversion Paradox in M-O-Kuaishou, observed in the Mixture-of-Experts (MoE) paradigm, presents several anomalies that negatively impact model performance. These anomalies can be categorized in a number of ways:\n",
            "\n",
            "1. **Reduced Diversity in the Training Data:**\n",
            "\n",
            "   *   The training data is typically a mixture of experts (experts with specific knowledge and skills) and novices (experts with no specific expertise).\n",
            "   *   The training data is often less diverse, leading to a smaller model, fewer diverse experts, and a lower representation of human-specific knowledge, potentially leading to more general\n",
            "-------------------------------------------------\n",
            "✅ ORIGINAL RESPONSE:\n",
            "The three anomalies observed are: (1) Expert Collapse, where some experts have significantly different output distributions and excessive zero activations, making it difficult for the gate networks to assign fair weights. (2) Expert Degradation, where shared experts are dominated by only one task, losing their ability to provide predictive information for all tasks. (3) Expert Underfitting, where data-sparse prediction tasks tend to ignore their specific experts and rely heavily on shared experts.\n",
            "=================================================\n",
            "\n",
            "❓ PROMPT:\n",
            "What is the main goal of the HoME (Hierarchy of Multi-Gate Experts) model proposed in the paper?\n",
            "-------------------------------------------------\n",
            "🤖 BASE MODEL RESPONSE:\n",
            "The main goal of the HoME (Hierarchy of Multi-Gate Experts) model is to **establish a framework for expertise in a specific area, enabling a more robust and efficient resource to provide better service to users.**\n",
            "\n",
            "-------------------------------------------------\n",
            "✅ ORIGINAL RESPONSE:\n",
            "The main goal of HoME is to achieve a simple, efficient, and balanced MoE system for multi-task learning that addresses the observed anomalies (expert collapse, degradation, and underfitting) to improve the stability and performance of the model.\n",
            "=================================================\n",
            "\n",
            "❓ PROMPT:\n",
            "Describe the \"Expert normalization & Swish mechanism\" used in HoME and explain why it is implemented.\n",
            "-------------------------------------------------\n",
            "🤖 BASE MODEL RESPONSE:\n",
            "HoME is a popular, yet complex, library, designed for usability, making it a good option for developers to experiment with. Here's a breakdown of the \"Expert Normalization & Swish Mechanism\" used in HoME:\n",
            "\n",
            "**The \"Expert Normalization & Swish Mechanism\" (for HoME):**\n",
            "\n",
            "This mechanism leverages a combination of several different techniques to ensure that the \"Expert Normalization & Swish\" process is consistently and efficiently applied to a wide range of inputs, regardless of whether they are already recognized as experts or not.\n",
            "\n",
            "**The Process:**\n",
            "\n",
            "1.  **Input Detection:**  The core\n",
            "-------------------------------------------------\n",
            "✅ ORIGINAL RESPONSE:\n",
            "The \"Expert normalization & Swish mechanism\" consists of two parts. First, each expert's output is normalized to approximate a standard normal distribution (mean of 0 and unit variance). This is done to balance the variance of expert outputs and prevent expert collapse. Secondly, the ReLU activation function is replaced with the Swish function to alleviate the zero derivatives gradient phenomenon that arises after normalization, improving parameter utilization and speeding up training.\n",
            "=================================================\n",
            "\n",
            "❓ PROMPT:\n",
            "Explain the \"Hierarchy mask mechanism\" in HoME and its purpose.\n",
            "-------------------------------------------------\n",
            "🤖 BASE MODEL RESPONSE:\n",
            "The Hierarchy Mask Mechanism (HMM) is a clever mechanism used in HoME to manipulate the behavior of the Behavior, Instruction, and Memory (B, I, M) system. It works by forcing the B, I, M system to behave in certain ways, usually to make the A system output \"better\" or \"more efficient\" output. This behavior is not always strictly enforced, and the goal is to make the A system output \"more interesting\" or \"efficient.\"\n",
            "\n",
            "Here's a breakdown of the Hierarchy Mask Mechanism (HMM):\n",
            "\n",
            "**Core Mechanism:**\n",
            "\n",
            "The HMM mechanism mimics the Behavior, Instruction,\n",
            "-------------------------------------------------\n",
            "✅ ORIGINAL RESPONSE:\n",
            "The \"Hierarchy mask mechanism\" introduces a pre-order meta expert network to group tasks based on prior relevance. In the Kuaishou example, tasks are divided into passive watching-time tasks and proactive interaction tasks. Each task then benefits from both fully-shared global experts and partially-shared in-category experts. The purpose is to reduce expert occupancy issues and mitigate expert degradation by allowing tasks to share experts within their category.\n",
            "=================================================\n",
            "\n",
            "❓ PROMPT:\n",
            "Describe the \"Feature-gate and Self-gate mechanisms\" and how they address the identified issues.\n",
            "-------------------------------------------------\n",
            "🤖 BASE MODEL RESPONSE:\n",
            "The \"Feature-gate and Self-gate mechanisms\" are a powerful combination of techniques that leverage the strengths of both Feature-gate and Self-gate mechanisms. They aim to extract and reveal nuanced and unexpected patterns from data, helping to identify and address issues that might be missed by traditional methods.\n",
            "\n",
            "Here's a breakdown of how they address the identified issues:\n",
            "\n",
            "**1. Feature-gate Mechanisms:**\n",
            "\n",
            "*   **Feature-gate mechanisms** are typically used to address issues with certain data types or specific data patterns. They use the \"Feature Gate\" (often denoted as `F`) to select specific types of data and,\n",
            "-------------------------------------------------\n",
            "✅ ORIGINAL RESPONSE:\n",
            "The \"Feature-gate mechanism\" provides private, flexible input to each expert to protect sparse-task expert training and prevent gradient conflicts among experts. The \"Self-gate mechanism\" is implemented by adding residual connections between adjacent related experts in deeper MMoE systems to ensure the top layers' gradient can effectively be passed to the bottom layers and stabilize the training of sparse-task experts, preventing gradient dilution.\n",
            "=================================================\n",
            "\n",
            "Clearing base model from memory...\n",
            "✅ Memory cleared.\n",
            "\n",
            "Loading fine-tuned model from: /content/drive/MyDrive/gemma3-270m-finetuned...\n",
            "==((====))==  Unsloth 2025.9.6: Fast Gemma3 patching. Transformers: 4.55.4.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n",
            "✅ Fine-tuned model loaded.\n",
            "\n",
            "--- Generating responses from FINE-TUNED MODEL ---\n",
            "\n",
            "❓ PROMPT:\n",
            "What are the three anomalies observed in the Mixture-of-Experts (MoE) paradigm at Kuaishou that negatively impact model performance?\n",
            "-------------------------------------------------\n",
            "🚀 FINE-TUNED MODEL RESPONSE:\n",
            "The three anomalies are: (i) Experts exhibit sequential gating, where experts load and reset data in parallel, and saving all retrieved parameters at once hinders model performance. (ii) Experts exhibit sequential reasoning, where experts load and reset data independently, and federated learning exercises fail to converge on unique experts. (iii) Expert behavior exhibits emergent dominance, where experts independently produce similar outputs, but together produce disjoint outputs, leading to cascading failures.\n",
            "-------------------------------------------------\n",
            "✅ ORIGINAL RESPONSE:\n",
            "The three anomalies observed are: (1) Expert Collapse, where some experts have significantly different output distributions and excessive zero activations, making it difficult for the gate networks to assign fair weights. (2) Expert Degradation, where shared experts are dominated by only one task, losing their ability to provide predictive information for all tasks. (3) Expert Underfitting, where data-sparse prediction tasks tend to ignore their specific experts and rely heavily on shared experts.\n",
            "=================================================\n",
            "\n",
            "❓ PROMPT:\n",
            "What is the main goal of the HoME (Hierarchy of Multi-Gate Experts) model proposed in the paper?\n",
            "-------------------------------------------------\n",
            "🚀 FINE-TUNED MODEL RESPONSE:\n",
            "The main goal of the HoME is to expand the capacity of the Mixture-of-Experts (MoE) paradigm, particularly in combinational language modeling (CLM) tasks, by integrating multiple expert models with diverse attack mechanisms. The key innovation of HoME is its lightweight, semantically-domain-aware fine-grained grouping mechanism, enabling the model to operate effectively on various expert subsets within a single MoE router.\n",
            "-------------------------------------------------\n",
            "✅ ORIGINAL RESPONSE:\n",
            "The main goal of HoME is to achieve a simple, efficient, and balanced MoE system for multi-task learning that addresses the observed anomalies (expert collapse, degradation, and underfitting) to improve the stability and performance of the model.\n",
            "=================================================\n",
            "\n",
            "❓ PROMPT:\n",
            "Describe the \"Expert normalization & Swish mechanism\" used in HoME and explain why it is implemented.\n",
            "-------------------------------------------------\n",
            "🚀 FINE-TUNED MODEL RESPONSE:\n",
            "The \"Expert normalization & Swish mechanism\" refers to the two parts of HoME: (1) the input embeddings used to guide the ranking of experts, and (2) the Smolet weighting function to equalize the average scores of all experts. The Smolet weighting function calculates a scaling factor wk for x. Experts with larger embeddings are smoothed as w ≈ 1 - k(x/d), while experts with smaller embeddings are weighted more strongly. Swish is implemented to combine these two components by dividing the scaled input embedding into a sum and multiplying it by the Smolet weighting.\n",
            "-------------------------------------------------\n",
            "✅ ORIGINAL RESPONSE:\n",
            "The \"Expert normalization & Swish mechanism\" consists of two parts. First, each expert's output is normalized to approximate a standard normal distribution (mean of 0 and unit variance). This is done to balance the variance of expert outputs and prevent expert collapse. Secondly, the ReLU activation function is replaced with the Swish function to alleviate the zero derivatives gradient phenomenon that arises after normalization, improving parameter utilization and speeding up training.\n",
            "=================================================\n",
            "\n",
            "❓ PROMPT:\n",
            "Explain the \"Hierarchy mask mechanism\" in HoME and its purpose.\n",
            "-------------------------------------------------\n",
            "🚀 FINE-TUNED MODEL RESPONSE:\n",
            "The \"Hierarchy mask mechanism\" in HoME decomposes complex tokens into representations that are locally interconnected. It introduces a set of independent invariant tokens (input tokens) to bridge the gap between the larger MoE model (Xue et al., 2023) and the specific task of this paper. The output tokens are defined by Equation 1.\n",
            "-------------------------------------------------\n",
            "✅ ORIGINAL RESPONSE:\n",
            "The \"Hierarchy mask mechanism\" introduces a pre-order meta expert network to group tasks based on prior relevance. In the Kuaishou example, tasks are divided into passive watching-time tasks and proactive interaction tasks. Each task then benefits from both fully-shared global experts and partially-shared in-category experts. The purpose is to reduce expert occupancy issues and mitigate expert degradation by allowing tasks to share experts within their category.\n",
            "=================================================\n",
            "\n",
            "❓ PROMPT:\n",
            "Describe the \"Feature-gate and Self-gate mechanisms\" and how they address the identified issues.\n",
            "-------------------------------------------------\n",
            "🚀 FINE-TUNED MODEL RESPONSE:\n",
            "The \"Feature-gate mechanism\" is implemented by masking expert outputs during the token embedding process. Self-gate mechanism is implemented by pruning less important experts.\n",
            "-------------------------------------------------\n",
            "✅ ORIGINAL RESPONSE:\n",
            "The \"Feature-gate mechanism\" provides private, flexible input to each expert to protect sparse-task expert training and prevent gradient conflicts among experts. The \"Self-gate mechanism\" is implemented by adding residual connections between adjacent related experts in deeper MMoE systems to ensure the top layers' gradient can effectively be passed to the bottom layers and stabilize the training of sparse-task experts, preventing gradient dilution.\n",
            "=================================================\n",
            "\n",
            "Clearing fine-tuned model from memory...\n",
            "✅ All done. Memory cleared.\n"
          ]
        }
      ]
    }
  ]
}